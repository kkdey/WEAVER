% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train_biword2vec.R
\name{train_biword2vec}
\alias{train_biword2vec}
\title{Train a model by bi-directional word2vec (biword2vec).}
\usage{
train_biword2vec(train_file, output_file_left = "vectors_left.bin",
  output_file_right = "vectors_right.bin",
  output_file_out = "vectors_out.bin", vectors = 100, threads = 3,
  window = 12, classes = 0, cbow = 0, min_count = 1, iter = 5,
  force = F, negative_samples = 0)
}
\arguments{
\item{train_file}{Path of a single .txt file for training. Tokens are split on spaces.}

\item{output_file_left}{Path of the output file for the left context words.}

\item{output_file_right}{Path of the output fle for the right context words.}

\item{vectors}{The number of vectors to output. Defaults to 100.
More vectors usually means more precision, but also more random error, higher memory usage, and slower operations.
Sensible choices are probably in the range 100-500.}

\item{threads}{Number of threads to run training process on.
Defaults to 1; up to the number of (virtual) cores on your machine may speed things up.}

\item{window}{The size of the window (in words) to use in training.}

\item{classes}{Number of classes for k-means clustering. Not documented/tested.}

\item{cbow}{If 1, use a continuous-bag-of-words model instead of skip-grams.
Defaults to false (recommended for newcomers).}

\item{min_count}{Minimum times a word must appear to be included in the samples.
High values help reduce model size.}

\item{iter}{Number of passes to make over the corpus in training.}

\item{force}{Whether to overwrite existing model files.}

\item{negative_samples}{Number of negative samples to take in skip-gram training. 0 means full sampling, while lower numbers
give faster training. For large corpora 2-5 may work; for smaller corpora, 5-15 is reasonable.}
}
\value{
A VectorSpaceModel object.
}
\description{
Trains a bi-directional word2vec model on a corpus or training data, which is in general a .txt
             file. A bi-directional word2vec model uses separate vector representations for the left
             and right context of a word. This enables the user to determine enrichment of association
             of a word with another in terms of context of use.
}
\examples{
\dontrun{
model = train_biword2vec("nation.txt", output_file_left = "out_left.bin",
                         output_file_right = "out_right.bin", threads = 3,
                         woindow = 5)
}
}
\references{
\url{https://code.google.com/p/word2vec/}
}
